{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08ecd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5595b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.16193581])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# sample data\n",
    "input_data = np.array([[2.5]], dtype=np.float32)\n",
    "\n",
    "# Specify the model name and version\n",
    "model_name = \"linear_regression_model\" #specified in config.pbtxt\n",
    "model_version = \"1\"\n",
    "\n",
    "# Set the inference URL based on the Triton server's address\n",
    "url = f\"http://localhost:8000/v2/models/{model_name}/versions/{model_version}/infer\"\n",
    "\n",
    "# payload with input params\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",  # what you named input in config.pbtxt\n",
    "            \"datatype\": \"FP32\",  \n",
    "            \"shape\": input_data.shape,\n",
    "            \"data\": input_data.tolist(),\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# sample invoke\n",
    "response = requests.post(url, data=json.dumps(payload))\n",
    "response.raise_for_status()\n",
    "\n",
    "# output result\n",
    "inference_result = response.json()\n",
    "output_data = np.array(inference_result[\"outputs\"][0][\"data\"])\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0be36e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'linear_regression_model',\n",
       " 'model_version': '1',\n",
       " 'outputs': [{'name': 'OUTPUT__0',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 1],\n",
       "   'data': [6.161935806274414]}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9ac090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup triton inference client\n",
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265a9743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tritonclient.http._infer_input.InferInput at 0x71bb9e1c6590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triton can infer the inputs from your config values\n",
    "inputs = httpclient.InferInput(\"INPUT__0\", input_data.shape, datatype=\"FP32\")\n",
    "inputs.set_data_from_numpy(input_data) #we set a numpy array in this case\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eb6359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tritonclient.http._requested_output.InferRequestedOutput at 0x71bb9e1c6f50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output configuration\n",
    "outputs = httpclient.InferRequestedOutput(\"OUTPUT__0\")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d03afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.161936]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample inference\n",
    "res = client.infer(model_name = \"linear_regression_model\", inputs=[inputs], outputs=[outputs], model_version=\"1\")\n",
    "inference_output = res.as_numpy('OUTPUT__0') #serialize numpy output\n",
    "inference_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0edbc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tritonclient.http._infer_result.InferResult"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea2d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 ms, sys: 2.01 ms, total: 14.5 ms\n",
      "Wall time: 29.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(100):\n",
    "    res = client.infer(model_name = \"linear_regression_model\", inputs=[inputs], outputs=[outputs], model_version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5075ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'triton',\n",
       " 'version': '2.37.0',\n",
       " 'extensions': ['classification',\n",
       "  'sequence',\n",
       "  'model_repository',\n",
       "  'model_repository(unload_dependents)',\n",
       "  'schedule_policy',\n",
       "  'model_configuration',\n",
       "  'system_shared_memory',\n",
       "  'cuda_shared_memory',\n",
       "  'binary_tensor_data',\n",
       "  'parameters',\n",
       "  'statistics',\n",
       "  'trace',\n",
       "  'logging']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_server_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191c850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'linear_regression_model',\n",
       " 'versions': ['1'],\n",
       " 'platform': 'pytorch_libtorch',\n",
       " 'inputs': [{'name': 'INPUT__0', 'datatype': 'FP32', 'shape': [1, 1]}],\n",
       " 'outputs': [{'name': 'OUTPUT__0', 'datatype': 'FP32', 'shape': [1, 1]}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_model_metadata(model_name=\"linear_regression_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87fc67f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6406c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85eb675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/models/sentiment, headers {}\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/json', 'Content-Length': '222'}>\n",
      "bytearray(b'{\"name\":\"sentiment\",\"versions\":[\"1\"],\"platform\":\"python\",\"inputs\":[{\"name\":\"text\",\"datatype\":\"BYTES\",\"shape\":[1]}],\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1]},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1]}]}')\n",
      "Updated Model Outputs:\n",
      "  - label: BYTES [1]\n",
      "  - score: FP32 [1]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new model metadata\n",
    "metadata = client.get_model_metadata(model_name=\"sentiment\")\n",
    "print(\"Updated Model Outputs:\")\n",
    "for output in metadata['outputs']:\n",
    "    print(f\"  - {output['name']}: {output['datatype']} {output['shape']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e5dece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "\n",
      "Text                                     Label      Confidence\n",
      "----------------------------------------------------------------------\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":30}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x1a\\x00\\x00\\x00I am super happy right now'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '241'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":11}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "I am super happy right now               5 stars    84.3%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":34}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x1e\\x00\\x00\\x00This is terrible and I hate it'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '240'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":10}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "This is terrible and I hate it           1 star     95.0%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":30}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x1a\\x00\\x00\\x00It\\'s okay, nothing special'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '241'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":11}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "It's okay, nothing special               3 stars    83.1%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":34}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x1e\\x00\\x00\\x00Absolutely amazing experience!'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '241'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":11}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "Absolutely amazing experience!           5 stars    96.3%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":20}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x10\\x00\\x00\\x00Worst thing ever'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '240'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":10}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "Worst thing ever                         1 star     96.1%\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '241'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":11}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "I am super happy right now               5 stars    84.3%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":34}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x1e\\x00\\x00\\x00This is terrible and I hate it'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '240'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":10}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "This is terrible and I hate it           1 star     95.0%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":30}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x1a\\x00\\x00\\x00It\\'s okay, nothing special'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '241'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":11}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "It's okay, nothing special               3 stars    83.1%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":34}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x1e\\x00\\x00\\x00Absolutely amazing experience!'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '241'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":11}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "Absolutely amazing experience!           5 stars    96.3%\n",
      "POST /v2/models/sentiment/infer, headers {'Inference-Header-Content-Length': 210}\n",
      "b'{\"inputs\":[{\"name\":\"text\",\"shape\":[1],\"datatype\":\"BYTES\",\"parameters\":{\"binary_data_size\":20}}],\"outputs\":[{\"name\":\"label\",\"parameters\":{\"binary_data\":true}},{\"name\":\"score\",\"parameters\":{\"binary_data\":true}}]}\\x10\\x00\\x00\\x00Worst thing ever'\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/octet-stream', 'Inference-Header-Content-Length': '226', 'Content-Length': '240'}>\n",
      "bytearray(b'{\"model_name\":\"sentiment\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"label\",\"datatype\":\"BYTES\",\"shape\":[1],\"parameters\":{\"binary_data_size\":10}},{\"name\":\"score\",\"datatype\":\"FP32\",\"shape\":[1],\"parameters\":{\"binary_data_size\":4}}]}')\n",
      "Worst thing ever                         1 star     96.1%\n"
     ]
    }
   ],
   "source": [
    "# Test the improved sentiment model with separate outputs\n",
    "test_texts = [\n",
    "    \"I am super happy right now\",\n",
    "    \"This is terrible and I hate it\",\n",
    "    \"It's okay, nothing special\",\n",
    "    \"Absolutely amazing experience!\",\n",
    "    \"Worst thing ever\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\\n\")\n",
    "print(f\"{'Text':<40} {'Label':<10} {'Confidence'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for text in test_texts:\n",
    "    # Prepare input\n",
    "    input_data = np.array([text], dtype=np.object_)\n",
    "    infer_input = httpclient.InferInput(\"text\", [1], \"BYTES\")\n",
    "    infer_input.set_data_from_numpy(input_data)\n",
    "    \n",
    "    # Request both outputs\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"label\"),\n",
    "        httpclient.InferRequestedOutput(\"score\")\n",
    "    ]\n",
    "    \n",
    "    # Run inference\n",
    "    result = client.infer(model_name=\"sentiment\", inputs=[infer_input], outputs=outputs)\n",
    "    \n",
    "    # Extract results - now it's super clean!\n",
    "    label = result.as_numpy(\"label\")[0].decode('utf-8')\n",
    "    score = result.as_numpy(\"score\")[0]\n",
    "    \n",
    "    print(f\"{text:<40} {label:<10} {score:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d13c54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/models/text_ensemble, headers {}\n",
      "<HTTPSocketPoolResponse status=200 headers={'Content-Type': 'application/json', 'Content-Length': '183'}>\n",
      "bytearray(b'{\"name\":\"text_ensemble\",\"versions\":[\"1\"],\"platform\":\"ensemble\",\"inputs\":[{\"name\":\"TEXT\",\"datatype\":\"BYTES\",\"shape\":[-1]}],\"outputs\":[{\"name\":\"LABEL\",\"datatype\":\"BYTES\",\"shape\":[-1]}]}')\n",
      "Model metadata:\n",
      "{'name': 'text_ensemble', 'versions': ['1'], 'platform': 'ensemble', 'inputs': [{'name': 'TEXT', 'datatype': 'BYTES', 'shape': [-1]}], 'outputs': [{'name': 'LABEL', 'datatype': 'BYTES', 'shape': [-1]}]}\n"
     ]
    }
   ],
   "source": [
    "# Check the model metadata to see what inputs/outputs are expected\n",
    "metadata = client.get_model_metadata(model_name=\"text_ensemble\")\n",
    "print(\"Model metadata:\")\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d138af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEGATIVE', 'POSITIVE', 'POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "client = httpclient.InferenceServerClient(\"localhost:8000\")\n",
    "\n",
    "# Prepare your text inputs\n",
    "texts = [\n",
    "    \"I am SO UPSET!\",\n",
    "    \"I am super happy!!\",\n",
    "    \"Life is great!\"\n",
    "]\n",
    "\n",
    "# Triton expects BYTES tensors for string inputs\n",
    "input_data = np.array([t.encode(\"utf-8\") for t in texts], dtype=object)\n",
    "\n",
    "# Create an InferInput matching the ensembleâ€™s config.pbtxt\n",
    "infer_input = httpclient.InferInput(\"TEXT\", [len(texts)], \"BYTES\")\n",
    "infer_input.set_data_from_numpy(input_data)\n",
    "\n",
    "# Perform inference on your ensemble\n",
    "response = client.infer(\n",
    "    model_name=\"text_ensemble\",\n",
    "    inputs=[infer_input]\n",
    ")\n",
    "\n",
    "# Retrieve outputs\n",
    "#   - If your ensemble outputs LABEL (postprocess stage)\n",
    "#   - Or LOGITS if you skipped postprocess\n",
    "try:\n",
    "    labels = response.as_numpy(\"LABEL\")\n",
    "    print([l.decode(\"utf-8\") for l in labels])\n",
    "except KeyError:\n",
    "    logits = response.as_numpy(\"LOGITS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9204770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test resnet50\n",
    "client = httpclient.InferenceServerClient(\"localhost:8000\")\n",
    "\n",
    "image = np.random.randint(low=0, high=255, size=(1, 3, 224, 224))\n",
    "image = image.astype(np.float32)\n",
    "\n",
    "infer_input = httpclient.InferInput(\"INPUT__0\", image.shape, \"FP32\")\n",
    "infer_input.set_data_from_numpy(image)\n",
    "\n",
    "response = client.infer(model_name=\"resnet50\", inputs=[infer_input])\n",
    "\n",
    "logits = response.as_numpy(\"OUTPUT__0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecf483ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25172845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: ['bucket']\n"
     ]
    }
   ],
   "source": [
    "# test resnet_ensemble\n",
    "\n",
    "client = httpclient.InferenceServerClient(\"localhost:8000\")\n",
    "\n",
    "image = np.random.randint(low=0, high=255, size=(3, 224, 224))\n",
    "image = image.astype(np.uint8)\n",
    "infer_input = httpclient.InferInput(\"INPUT__0\", image.shape, \"UINT8\")\n",
    "infer_input.set_data_from_numpy(image)\n",
    "response = client.infer(model_name=\"resnet_ensemble\", inputs=[infer_input])\n",
    "\n",
    "# Get labels and decode from bytes to string\n",
    "labels_bytes = response.as_numpy(\"LABELS\")\n",
    "labels = [label.decode('utf-8') for label in labels_bytes]\n",
    "print(f\"Predicted labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0582e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
