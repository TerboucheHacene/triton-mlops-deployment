
services:
  triton:
    # image: nvcr.io/nvidia/tritonserver:23.08-py3
    image: triton-python-backend:latest
    container_name: triton_server
    runtime: nvidia
    shm_size: '1gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "8000:8000"   # HTTP
      - "8001:8001"   # gRPC
      - "8002:8002"   # Metrics
    volumes:
      - /home/hacene/Documents/workspace/MLOps/model_deployment/model_repository:/models
    command: >
      tritonserver
      --model-repository=/models
      --exit-on-error=false
      --log-verbose=1
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility